{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ff781e6-0c0f-45da-a6ac-cbdc74b3765b",
   "metadata": {},
   "source": [
    "## High-Level ETL (Extract - Transform - Load) Flow\n",
    "**Goal**: By the end of this tutorial, you will be able to\n",
    "- Extract: Download a file from AWS S3 using Pythonâ€™s boto3.\n",
    "- Transform: Clean, filter, or manipulate data in Python (often using libraries like pandas).\n",
    "- Load: Insert the transformed data into a relational database via SQL statements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4c364-5538-4205-bc71-4a9e631eeefc",
   "metadata": {},
   "source": [
    "## Lab Assignment\n",
    "\n",
    "1. Implement the following functions\n",
    "   - `extract_from_csv(file_to_process: str) -> pd.DataFrame`: read the .csv file and return dataframe\n",
    "   - `extract_from_json(file_to_process: str) -> pd.DataFrame`: read the .json file and return dataframe\n",
    "   - `extract() -> pd.DataFrame`: extract data of heterogeneous format and combine them into a single dataframe.\n",
    "   - `transform(df) -> pd.DataFrame`: function for data cleaning and manipulation.\n",
    "2. Clean the data\n",
    "   - Round float-type columns to two decimal places.\n",
    "   - remove duplicate samples\n",
    "   - Save the cleaned data into parquet file\n",
    "3. Insert the data into SQL\n",
    "   - Create postgresql database\n",
    "   - Insert the data into the database\n",
    "  \n",
    "Submission requirement:\n",
    "    1. Jupyter Notebook\n",
    "    2. Parquet File\n",
    "    3. SQL file (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09576b3b-7b35-462f-a323-2e1252518a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading sqlalchemy-2.0.40-cp311-cp311-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting greenlet>=1 (from sqlalchemy)\n",
      "  Downloading greenlet-3.2.1-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\rache\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (4.13.2)\n",
      "Downloading sqlalchemy-2.0.40-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.1 MB 165.2 kB/s eta 0:00:13\n",
      "    --------------------------------------- 0.0/2.1 MB 220.2 kB/s eta 0:00:10\n",
      "   - -------------------------------------- 0.1/2.1 MB 299.4 kB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.1/2.1 MB 422.8 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.2/2.1 MB 765.3 kB/s eta 0:00:03\n",
      "   ------ --------------------------------- 0.3/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.3/2.1 MB 1.1 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.4/2.1 MB 933.2 kB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.6/2.1 MB 1.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 0.8/2.1 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.9/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.0/2.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.2/2.1 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.4/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.6/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.7/2.1 MB 2.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.8/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.9/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.0/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 2.1 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.2.1-cp311-cp311-win_amd64.whl (295 kB)\n",
      "   ---------------------------------------- 0.0/295.4 kB ? eta -:--:--\n",
      "   ------------------- -------------------- 143.4/295.4 kB 2.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 286.7/295.4 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 295.4/295.4 kB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.2.1 sqlalchemy-2.0.40\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f67f5058-8012-44d7-8480-007ebc7beb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Package:\n",
    "# psycopg2 2.9.10 (A PostgreSQL database adapter)\n",
    "# pandas 2.0.3 (For data manipulation and analysis)\n",
    "# sqlalchemy 2.0.37 (A SQL toolkit and Object Relational Mapper)\n",
    "# pyarrow 14.0.1 (Provides support for efficient in-memory columnar data structures, part from Apache Arrow Objective)\n",
    "#!pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "#required for reading .xml files\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#required for navigating machine's directory\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "#required for communicating with SQL database\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26ea5ad-4aa4-4d75-8cca-41de1e7454f3",
   "metadata": {},
   "source": [
    "# E: Extracting data from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5964aa80-e1d5-42bf-8159-6ad91273a192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "my_aws_access_key_id='REDACTED'\n",
    "my_aws_secret_access_key='REDACTED'\n",
    "my_aws_session_token='IQoJb3JpZ2luX2VjEKb//////////wEaCXVzLWVhc3QtMiJHMEUCIQC3AUjfF2sFmuTLKRmc58vroT2mLkWJIPH7ee691QsxJgIgGyPopwo2vrlXG/ETq5JThT6zeZd5D2uQr2mMePjrzBQq6wIITxAAGgw1NDk3ODcwOTAwMDgiDKlfPtD7jxAHsv2OUSrIArqcSGr4ViAAku+ZvwdAJfnJhoFq7RZ465FJXaxdY138xEJZBJDSlO6naedAVGwKNsh5KImFsgndonGQpz35m5bxyexgn+rLRLt183bJGFeJz0t9D3TYdwv0h+w8XQXCH2IYyUXMIGldCpSNMBzyPWYKrQrBBMQt7f2fZwYnVQfjOOkX66LDd7B1OwiYsmggzJRHtfXFkXJoShJVCytAAxNUc1aOdCdAMPzshnadyvs7LoWDqqkOmlzzDIkPUbQUBrSvj9R3fUEX4paPXeGTxvn5w0yYck6U7j9sXb6Jxf7Sdp4Rn5ESqg/ONEJ80nqmSHXhys0CA9IOdsaKpFRUKnyqA1Oe/mF0f+H5k2zah2JejDc6eqO73mVeaSX4OfZ49yZNS4XPlKylF5Qw94PkdX6fqp2ADmH1SnOt7hYCE6fUbCUNgLLm6sUw64/qwAY6pwGiuz36QgvmRX1YzxaxPpdEuyXhTZ596Xi/l0WmDaGM3dytXIdz3X5Vbf9qhFqE4ivPP27isAFy8kNg0xeD5J5ZOCT9cxMRx60P9sifYr4dxnf6d5dc3dnPGlF+ulMKLFI9TlnP3NERwF81klWtOR4YIf0Qg9jV0+PmVEzM0bhncTgLX926TdNo0UN2w2k91wxpQIPHaBu3vH623FkS3FMdP1hBlGwpWg=='\n",
    "\n",
    "BUCKET_NAME = 'de300spring2025'   # Replace with your bucket name\n",
    "S3_FOLDER = 'dinglin_xia/lab4_data/'             # The folder path in S3\n",
    "LOCAL_DIR = './local-data/'      # Local directory to save files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3161fa0-3999-4aff-a1e8-e0f364fd5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_s3_folder(bucket_name, s3_folder, local_dir):\n",
    "    \"\"\"Download a folder from S3.\"\"\"\n",
    "    if not os.path.exists(local_dir):\n",
    "        os.makedirs(local_dir)\n",
    "\n",
    "    # List objects within the specified folder\n",
    "    s3_resource = boto3.resource('s3',\n",
    "                                aws_access_key_id=my_aws_access_key_id,\n",
    "                                aws_secret_access_key=my_aws_secret_access_key,\n",
    "                                aws_session_token=my_aws_session_token)\n",
    "    bucket = s3_resource.Bucket(bucket_name)\n",
    "    \n",
    "    for obj in bucket.objects.filter(Prefix=s3_folder):\n",
    "        # Define local file path\n",
    "        local_file_path = os.path.join(local_dir, obj.key[len(s3_folder):])  \n",
    "        \n",
    "        if obj.key.endswith('/'):  # Skip folders\n",
    "            continue\n",
    "        \n",
    "        # Create local directory if needed\n",
    "        local_file_dir = os.path.dirname(local_file_path)\n",
    "        if not os.path.exists(local_file_dir):\n",
    "            os.makedirs(local_file_dir)\n",
    "        \n",
    "        # Download the file\n",
    "        bucket.download_file(obj.key, local_file_path)\n",
    "        print(f\"Downloaded {obj.key} to {local_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca57a83a-ceb6-4cb8-9101-75426a5728f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded dinglin_xia/lab4_data/used_car_prices1.csv to ./local-data/used_car_prices1.csv\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices1.json to ./local-data/used_car_prices1.json\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices1.xml to ./local-data/used_car_prices1.xml\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices2.csv to ./local-data/used_car_prices2.csv\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices2.json to ./local-data/used_car_prices2.json\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices2.xml to ./local-data/used_car_prices2.xml\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices3.csv to ./local-data/used_car_prices3.csv\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices3.json to ./local-data/used_car_prices3.json\n",
      "Downloaded dinglin_xia/lab4_data/used_car_prices3.xml to ./local-data/used_car_prices3.xml\n"
     ]
    }
   ],
   "source": [
    "download_s3_folder(BUCKET_NAME, S3_FOLDER, LOCAL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14027a8-8a6d-4566-93ba-710e68b97f3e",
   "metadata": {},
   "source": [
    "## Extract data from ./data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03b2bdcb-fb78-419c-a3e9-9f12ee8f2b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob('./data/*')\n",
    "\n",
    "# Output the list of files\n",
    "for file in all_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c30b5a8-7206-4a8c-8837-b42ab76115fb",
   "metadata": {},
   "source": [
    "### Function to extract data from one .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac06a7b-ea5d-4410-b0b3-be9db100d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(file_to_process: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(file_to_process)\n",
    "    # add you line here to read the .csv file and return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7051e6b2-6f3a-4b2c-9ff4-1e10a91a296e",
   "metadata": {},
   "source": [
    "### Function to extract data from one .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31b19053-3d8a-4386-82d9-5a7b287d37db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_json(file_to_process: str) -> pd.DataFrame:\n",
    "    return pd.read_json(file_to_process, lines=True)\n",
    "    # add you line here to read the .json file and return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072dfa48-6805-451e-9d0f-eb09de70e9b1",
   "metadata": {},
   "source": [
    "### Function to extract data from one  .xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f6bd897-d626-43f3-9980-e3c1e0e27ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_xml(file_to_process: str) -> pd.DataFrame:\n",
    "    dataframe = pd.DataFrame(columns = columns)\n",
    "    tree = ET.parse(file_to_process)\n",
    "    root = tree.getroot()\n",
    "    for person in root:\n",
    "        car_model = person.find(\"car_model\").text\n",
    "        year_of_manufacture = int(person.find(\"year_of_manufacture\").text)\n",
    "        price = float(person.find(\"price\").text)\n",
    "        fuel = person.find(\"fuel\").text\n",
    "        sample = pd.DataFrame({\"car_model\":car_model, \"year_of_manufacture\":year_of_manufacture, \"price\":price, \"fuel\":fuel}, index = [0])\n",
    "        dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2e693-0d74-4e9e-92e1-395119aab10e",
   "metadata": {},
   "source": [
    "### Function to extract data from the ./data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fbc266d-1ef2-49bb-8784-4a8ccbffd038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract() -> pd.DataFrame:\n",
    "    extracted_data = pd.DataFrame(columns = columns)\n",
    "    #for csv files\n",
    "    for csv_file in glob.glob(os.path.join(folder, \"*.csv\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_csv(csv_file)], ignore_index=True)\n",
    "\n",
    "    #add lines for json files\n",
    "    for json_file in glob.glob(os.path.join(folder, \"*.json\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_json(json_file)], ignore_index=True)\n",
    "\n",
    "    #add lines for xml files\n",
    "    for xml_file in glob.glob(os.path.join(folder, \"*.xml\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_xml(xml_file)], ignore_index=True)\n",
    "\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cafa18-245e-4e2c-a63b-6854eb93b863",
   "metadata": {},
   "source": [
    "### Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a70a8ce-348f-4b26-8a9a-3ead42565c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rache\\AppData\\Local\\Temp\\ipykernel_56024\\1217209255.py:5: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  extracted_data = pd.concat([extracted_data, extract_from_csv(csv_file)], ignore_index=True)\n",
      "C:\\Users\\rache\\AppData\\Local\\Temp\\ipykernel_56024\\3387287639.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
      "C:\\Users\\rache\\AppData\\Local\\Temp\\ipykernel_56024\\3387287639.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
      "C:\\Users\\rache\\AppData\\Local\\Temp\\ipykernel_56024\\3387287639.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "columns = ['car_model','year_of_manufacture','price', 'fuel']\n",
    "folder = \"local-data\"\n",
    "#table_name = \"car_data\"\n",
    "\n",
    "# run\n",
    "def main():\n",
    "    data = extract()\n",
    "    #insert_to_table(data, \"car_data\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b1288a8-86fb-498c-833a-682ecd9c1eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_model</th>\n",
       "      <th>year_of_manufacture</th>\n",
       "      <th>price</th>\n",
       "      <th>fuel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ritz</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sx4</td>\n",
       "      <td>2013</td>\n",
       "      <td>7089.552239</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ciaz</td>\n",
       "      <td>2017</td>\n",
       "      <td>10820.895522</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wagon r</td>\n",
       "      <td>2011</td>\n",
       "      <td>4253.731343</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swift</td>\n",
       "      <td>2014</td>\n",
       "      <td>6865.671642</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car_model year_of_manufacture         price    fuel\n",
       "0      ritz                2014   5000.000000  Petrol\n",
       "1       sx4                2013   7089.552239  Diesel\n",
       "2      ciaz                2017  10820.895522  Petrol\n",
       "3   wagon r                2011   4253.731343  Petrol\n",
       "4     swift                2014   6865.671642  Diesel"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5265c408-8c63-413f-ad04-8d391e5b564d",
   "metadata": {},
   "source": [
    "# T: Transformation data and save organized data to .parquet file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faae9e13-ded5-47e0-8316-13ec6084ed56",
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_file = \"cars.parquet\"\n",
    "staging_data_dir = \"staging_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eefdc72e-6359-4363-a648-86082888f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(df):\n",
    "    print(f\"Shape of data {df.shape}\")\n",
    "\n",
    "    # truncate price with 2 decimal place (add your code below)\n",
    "    df['price'] = df['price'].astype(float).round(2)\n",
    "    \n",
    "    # remove samples with same car_model (add your code below)\n",
    "    df = df.drop_duplicates(subset='car_model')\n",
    "    \n",
    "    print(f\"Shape of data {df.shape}\")\n",
    "\n",
    "    # Ensure the staging directory exists before writing the Parquet file\n",
    "    if not os.path.exists(staging_data_dir):\n",
    "        os.makedirs(staging_data_dir)\n",
    "        print(f\"Directory '{staging_data_dir}' created.\")\n",
    "\n",
    "    # write to parquet\n",
    "    df.to_parquet(os.path.join(staging_data_dir, staging_file))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f5d5423-c198-4aa7-89f7-354992f97324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data (90, 4)\n",
      "Shape of data (25, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>car_model</th>\n",
       "      <th>year_of_manufacture</th>\n",
       "      <th>price</th>\n",
       "      <th>fuel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ritz</td>\n",
       "      <td>2014</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sx4</td>\n",
       "      <td>2013</td>\n",
       "      <td>7089.55</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ciaz</td>\n",
       "      <td>2017</td>\n",
       "      <td>10820.90</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>wagon r</td>\n",
       "      <td>2011</td>\n",
       "      <td>4253.73</td>\n",
       "      <td>Petrol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swift</td>\n",
       "      <td>2014</td>\n",
       "      <td>6865.67</td>\n",
       "      <td>Diesel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  car_model year_of_manufacture     price    fuel\n",
       "0      ritz                2014   5000.00  Petrol\n",
       "1       sx4                2013   7089.55  Diesel\n",
       "2      ciaz                2017  10820.90  Petrol\n",
       "3   wagon r                2011   4253.73  Petrol\n",
       "4     swift                2014   6865.67  Diesel"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the head of your data\n",
    "df = transform(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a960d531-a731-42d4-906c-4602391a16ca",
   "metadata": {},
   "source": [
    "# L: Loading data for further modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef01f8d-a8b6-454c-a842-61c44cf04efc",
   "metadata": {},
   "source": [
    "### Set Up PostgreSQL Locally\n",
    "#### Step 1: Install PostgreSQL\n",
    "- Windows: Download from MySQL Official Site {https://www.postgresql.org/download/}\n",
    "- Mac:\n",
    "  ```{bash}\n",
    "  brew install postgresql\n",
    "  brew services start postgresql\n",
    "  ```\n",
    "Then access PostgreSQL CLI\n",
    "```{bash}\n",
    "psql -U postgres\n",
    "```\n",
    "Note: if you don't have default \"postgres\" user, then create it manually by\n",
    "```{bash}\n",
    "default \"postgres\" user\n",
    "```\n",
    "or\n",
    "```{bash}\n",
    "sudo -u $(whoami) createuser postgres -s\n",
    "```\n",
    "\n",
    "Then create a database\n",
    "```{sql}\n",
    "CREATE DATABASE my_local_db;\n",
    "\\l  -- List all databases\n",
    "```\n",
    "\n",
    "#### Step 2: Create a User and Grant Privileges\n",
    "In PostgreSQL CLI:\n",
    "```{sql}\n",
    "CREATE USER myuser WITH ENCRYPTED PASSWORD 'mypassword';\n",
    "GRANT ALL PRIVILEGES ON DATABASE my_local_db TO myuser;\n",
    "```\n",
    "\n",
    "#### Step 3: Install Required Python Libraries\n",
    "```{bash}\n",
    "pip install pandas sqlalchemy pymysql psycopg2 mysql-connector-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b270a29e-c73a-4ca1-b7e8-64ec621cb29a",
   "metadata": {},
   "source": [
    "### Utility function for writing data into the SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f188de2-ae27-444c-8590-4c1dde9ac7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\rache\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\rache\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.40)\n",
      "Collecting pymysql\n",
      "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting psycopg2\n",
      "  Downloading psycopg2-2.9.10-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.3.0-cp311-cp311-win_amd64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\rache\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rache\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rache\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rache\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\rache\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\rache\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sqlalchemy) (4.13.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rache\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/45.0 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 10.2/45.0 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 10.2/45.0 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 41.0/45.0 kB 393.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 45.0/45.0 kB 318.6 kB/s eta 0:00:00\n",
      "Downloading psycopg2-2.9.10-cp311-cp311-win_amd64.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.2 MB 3.4 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.2/1.2 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.4/1.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.6/1.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 0.7/1.2 MB 3.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 0.9/1.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.0/1.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.2/1.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.2/1.2 MB 3.4 MB/s eta 0:00:00\n",
      "Downloading mysql_connector_python-9.3.0-cp311-cp311-win_amd64.whl (16.4 MB)\n",
      "   ---------------------------------------- 0.0/16.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/16.4 MB 6.3 MB/s eta 0:00:03\n",
      "    --------------------------------------- 0.4/16.4 MB 5.0 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.6/16.4 MB 3.9 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.7/16.4 MB 4.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.9/16.4 MB 4.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 1.1/16.4 MB 4.2 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.4/16.4 MB 4.1 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 1.6/16.4 MB 4.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.8/16.4 MB 4.4 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 2.0/16.4 MB 4.5 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.1/16.4 MB 4.2 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 2.3/16.4 MB 4.1 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.5/16.4 MB 4.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 2.9/16.4 MB 4.3 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 3.1/16.4 MB 4.4 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 3.4/16.4 MB 4.5 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.7/16.4 MB 4.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 3.9/16.4 MB 4.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 4.2/16.4 MB 4.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.5/16.4 MB 4.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 4.7/16.4 MB 4.8 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 5.1/16.4 MB 4.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.4/16.4 MB 5.0 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 5.6/16.4 MB 5.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.8/16.4 MB 5.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 6.1/16.4 MB 5.1 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 6.4/16.4 MB 5.1 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 6.7/16.4 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.0/16.4 MB 5.2 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.3/16.4 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 7.5/16.4 MB 5.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 7.9/16.4 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.2/16.4 MB 5.4 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 8.5/16.4 MB 5.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 8.7/16.4 MB 5.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 9.0/16.4 MB 5.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 9.3/16.4 MB 5.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 9.5/16.4 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 9.9/16.4 MB 5.4 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 10.2/16.4 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 10.5/16.4 MB 5.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 10.9/16.4 MB 5.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 11.2/16.4 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.6/16.4 MB 5.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 11.9/16.4 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 12.2/16.4 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 12.4/16.4 MB 6.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 12.7/16.4 MB 6.4 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 13.0/16.4 MB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 13.4/16.4 MB 6.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 13.7/16.4 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.1/16.4 MB 6.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 14.4/16.4 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.7/16.4 MB 6.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 15.1/16.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.5/16.4 MB 6.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 15.7/16.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.0/16.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.3/16.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  16.4/16.4 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.4/16.4 MB 6.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pymysql, psycopg2, mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.3.0 psycopg2-2.9.10 pymysql-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas sqlalchemy pymysql psycopg2 mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9de0546-846c-442f-bb89-324c66114e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database credentials\n",
    "db_host = \"localhost\"\n",
    "db_user = \"myuser\"\n",
    "db_password = \"mypassword\"\n",
    "db_name = \"my_local_db\"\n",
    "\n",
    "conn_string = f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}/{db_name}\"\n",
    "\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cdb749a-4467-4264-b23b-f0693ee980af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            schemaname                tablename tableowner tablespace  \\\n",
      "0           pg_catalog             pg_statistic   postgres       None   \n",
      "1           pg_catalog                  pg_type   postgres       None   \n",
      "2           pg_catalog         pg_foreign_table   postgres       None   \n",
      "3           pg_catalog                pg_authid   postgres  pg_global   \n",
      "4           pg_catalog    pg_statistic_ext_data   postgres       None   \n",
      "..                 ...                      ...        ...        ...   \n",
      "63          pg_catalog           pg_largeobject   postgres       None   \n",
      "64  information_schema                sql_parts   postgres       None   \n",
      "65  information_schema             sql_features   postgres       None   \n",
      "66  information_schema  sql_implementation_info   postgres       None   \n",
      "67  information_schema               sql_sizing   postgres       None   \n",
      "\n",
      "    hasindexes  hasrules  hastriggers  rowsecurity  \n",
      "0         True     False        False        False  \n",
      "1         True     False        False        False  \n",
      "2         True     False        False        False  \n",
      "3         True     False        False        False  \n",
      "4         True     False        False        False  \n",
      "..         ...       ...          ...          ...  \n",
      "63        True     False        False        False  \n",
      "64       False     False        False        False  \n",
      "65       False     False        False        False  \n",
      "66       False     False        False        False  \n",
      "67       False     False        False        False  \n",
      "\n",
      "[68 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Test connection\n",
    "df = pd.read_sql(\"SELECT * FROM pg_catalog.pg_tables;\", con=engine)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2fed596-5b37-4da6-927d-facc292e733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_to_table(data: pd.DataFrame, conn_string:str, table_name:str):\n",
    "    db = create_engine(conn_string) # creates a connection to the database using SQLAlchemy\n",
    "    conn = db.connect() # Establishes a database connection\n",
    "    data.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db20ac0d-c828-45a1-b29a-a8cf20e5cdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 4)\n"
     ]
    }
   ],
   "source": [
    "# read from the .parquet file\n",
    "\n",
    "def load() -> pd.DataFrame:\n",
    "    data = pd.DataFrame()\n",
    "    for parquet_file in glob.glob(os.path.join(staging_data_dir, \"*.parquet\")):\n",
    "        data = pd.concat([pd.read_parquet(parquet_file),data])\n",
    "\n",
    "    #insert_to_table(data, table_name)\n",
    "    insert_to_table(data = data, conn_string = conn_string, table_name = 'ml_car_data')\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load()\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63ed4a-d198-4b5e-a762-d937a7072c85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
