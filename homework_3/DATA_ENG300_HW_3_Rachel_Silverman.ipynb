{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1\n"
      ],
      "metadata": {
        "id": "LWyZELc28OFI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7OA8Y7d28DIt",
        "outputId": "bf7b9ba5-d947-413a-ffad-052443e2dec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 33.2M  100 33.2M    0     0  16.5M      0  0:00:02  0:00:02 --:--:-- 16.5M\n"
          ]
        }
      ],
      "source": [
        "# Load in the data\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Begin spark session\n",
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"AG news\")\n",
        "         .getOrCreate()\n",
        "        )\n",
        "\n",
        "# Read data\n",
        "agnews = spark.read.csv(\"agnews_clean.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# Turning the second column from a string to an array\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
      ],
      "metadata": {
        "id": "RPG7mWOJ8NfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Each row contains the document id and a list of filtered words\n",
        "agnews.show(5, truncate=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mra0_QRM8W9y",
        "outputId": "cc0c4c5e-63e3-480b-926c-d4f0e73b31dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------+\n",
            "|_c0|                      filtered|\n",
            "+---+------------------------------+\n",
            "|  0|[wall, st, bears, claw, bac...|\n",
            "|  1|[carlyle, looks, toward, co...|\n",
            "|  2|[oil, economy, cloud, stock...|\n",
            "|  3|[iraq, halts, oil, exports,...|\n",
            "|  4|[oil, prices, soar, time, r...|\n",
            "+---+------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import size\n",
        "\n",
        "# Explode the data such that each word gets its own row in order to be analyzed more conveniently\n",
        "words = agnews.select('_c0', explode(col('filtered')).alias('word'))\n",
        "\n",
        "words.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oe1ggbO7w2M-",
        "outputId": "75a30024-0dc7-42ef-d0de-f60e51a4f820"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "|_c0| word|\n",
            "+---+-----+\n",
            "|  0| wall|\n",
            "|  0|   st|\n",
            "|  0|bears|\n",
            "|  0| claw|\n",
            "|  0| back|\n",
            "+---+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by the document id and then word and aggregate by counting the number occurrences; result is the word count by document\n",
        "word_counts_per_document = (words\n",
        "                            .groupBy('_c0', 'word')\n",
        "                            .count()\n",
        "                            .withColumnRenamed('count', 'word_count'))\n",
        "\n",
        "# Show the result, compare against document 0 to validate that it is working properly\n",
        "word_counts_per_document.orderBy('_c0', ascending = True).show(18)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkqyssZYxoQ8",
        "outputId": "4bac2959-34b7-4c95-f9cd-3a0943a47679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----------+\n",
            "|_c0|      word|word_count|\n",
            "+---+----------+----------+\n",
            "|  0|        st|         1|\n",
            "|  0|    cynics|         1|\n",
            "|  0|     ultra|         1|\n",
            "|  0|     green|         1|\n",
            "|  0|    street|         1|\n",
            "|  0|     bears|         1|\n",
            "|  0|    seeing|         1|\n",
            "|  0|      band|         1|\n",
            "|  0|   sellers|         1|\n",
            "|  0|      claw|         1|\n",
            "|  0|      wall|         2|\n",
            "|  0|   reuters|         2|\n",
            "|  0|     black|         1|\n",
            "|  0| dwindling|         1|\n",
            "|  0|     short|         1|\n",
            "|  0|      back|         1|\n",
            "|  1|    toward|         1|\n",
            "|  1|investment|         1|\n",
            "+---+----------+----------+\n",
            "only showing top 18 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sum the word counts of all words in each document; result is the number of words in each document\n",
        "total_words_per_document = (word_counts_per_document\n",
        "                            .groupBy('_c0')\n",
        "                            .sum('count')\n",
        "                            .withColumnRenamed('sum(count)', 'words_in_doc'))\n",
        "\n",
        "# Show the results and validate that it is wordking properly\n",
        "total_words_per_document.orderBy('_c0', ascending = True).show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKbDXpf8xwzA",
        "outputId": "e6b34299-1f18-46d0-b387-bbced3244228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+\n",
            "|_c0|words_in_doc|\n",
            "+---+------------+\n",
            "|  0|          18|\n",
            "|  1|          27|\n",
            "|  2|          24|\n",
            "|  3|          28|\n",
            "|  4|          30|\n",
            "|  5|          32|\n",
            "|  6|          30|\n",
            "|  7|          30|\n",
            "|  8|          44|\n",
            "|  9|          19|\n",
            "+---+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Group by the word to determine if the word appears in that document that count over all documents; result is the number of documents in which that word appears\n",
        "documents_per_word = (\n",
        "    word_counts_per_document\n",
        "    .groupBy('word')\n",
        "    .count()\n",
        "    .withColumnRenamed('count', 'document_count')\n",
        "    .orderBy('document_count', ascending=False)\n",
        ")\n",
        "\n",
        "# Display results\n",
        "documents_per_word.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W94MartJGNsN",
        "outputId": "9de662f3-cd61-4b40-b1a3-2b0bb01ca9fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------+\n",
            "|   word|document_count|\n",
            "+-------+--------------+\n",
            "|     39|         31734|\n",
            "|   said|         20141|\n",
            "|    new|         18950|\n",
            "|reuters|         13750|\n",
            "|     us|         10424|\n",
            "|    two|          9657|\n",
            "|  first|          9193|\n",
            "|   year|          9116|\n",
            "|     ap|          9040|\n",
            "| monday|          7855|\n",
            "+-------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Join word counts by document data with total words in the document on the document id\n",
        "combined = word_counts_per_document.join(total_words_per_document, '_c0')\n",
        "\n",
        "# Join the previous dataframe with the number of documents in which each word appears for a complete dataframe of all needed data\n",
        "combined_all = combined.join(documents_per_word, 'word')\n",
        "\n",
        "# Show the results to validate that all columns were combined correctly into one dataframe\n",
        "combined_all.orderBy('_c0', ascending = True).show(18)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqzyUNL9D9_7",
        "outputId": "85e9047d-5acf-4202-ab8e-e762d5c3b2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---+----------+------------+--------------+\n",
            "|     word|_c0|word_count|words_in_doc|document_count|\n",
            "+---------+---+----------+------------+--------------+\n",
            "|dwindling|  0|         1|          18|            34|\n",
            "|       st|  0|         1|          18|          1217|\n",
            "|     back|  0|         1|          18|          4233|\n",
            "|    ultra|  0|         1|          18|            76|\n",
            "|    green|  0|         1|          18|           719|\n",
            "|   street|  0|         1|          18|          1502|\n",
            "|    bears|  0|         1|          18|           295|\n",
            "|   seeing|  0|         1|          18|           143|\n",
            "|   cynics|  0|         1|          18|             5|\n",
            "|  sellers|  0|         1|          18|            41|\n",
            "|     band|  0|         1|          18|           181|\n",
            "|     wall|  0|         2|          18|          1277|\n",
            "|     claw|  0|         1|          18|            16|\n",
            "|    black|  0|         1|          18|           627|\n",
            "|  reuters|  0|         2|          18|         13750|\n",
            "|    short|  0|         1|          18|           867|\n",
            "|    group|  1|         1|          27|          4404|\n",
            "|   toward|  1|         1|          27|           757|\n",
            "+---------+---+----------+------------+--------------+\n",
            "only showing top 18 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dataframe to an RDD for mapping and reducing functions to be applied\n",
        "combined_all_rdd = combined_all.select('_c0', 'word', 'word_count', 'words_in_doc', 'document_count').rdd\n",
        "\n",
        "# Show the first few rows to validate that it converted correctly\n",
        "combined_all_rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4vS-U6WnC86",
        "outputId": "f3ddb230-0314-43ee-f517-80007d6d12fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(_c0=42468, word='online', word_count=1, words_in_doc=26, document_count=2444),\n",
              " Row(_c0=45307, word='online', word_count=1, words_in_doc=28, document_count=2444),\n",
              " Row(_c0=23364, word='online', word_count=1, words_in_doc=13, document_count=2444),\n",
              " Row(_c0=36538, word='still', word_count=1, words_in_doc=58, document_count=2281),\n",
              " Row(_c0=26425, word='still', word_count=1, words_in_doc=31, document_count=2281)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of documents total in agnews; result to be used as a constant in analysis\n",
        "num_docs = agnews.count()\n",
        "\n",
        "# Display rsult\n",
        "num_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEnVlJkiJwVb",
        "outputId": "f49e0e79-eeec-4a54-dd9a-1baa7d0fc769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "127600"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log\n",
        "\n",
        "# Compute TF-IDF per word by mapping a function onto each row in the rdd and performing operations according to the given formula\n",
        "tfidf = combined_all_rdd.map(lambda row: (\n",
        "    row._c0,\n",
        "    {'word': row.word, 'tfidf': (row.word_count / row.words_in_doc) * log(num_docs / row.document_count)}\n",
        "))\n",
        "\n",
        "# Combine results for repeating words in documents and track the TF-IDF by document and word using the groupByKey and convert result to a list\n",
        "tfidf_grouped = tfidf.groupByKey().mapValues(list).sortByKey(ascending=True)\n",
        "\n",
        "# Display the TF-IDF values for each word in the first 5 documents\n",
        "tfidf_grouped.take(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Tj2vrMEO4g0",
        "outputId": "5a20a4a2-b110-45ae-84d8-f5355f557137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  [{'word': 'cynics', 'tfidf': 0.563734318747707},\n",
              "   {'word': 'green', 'tfidf': 0.2877107940095433},\n",
              "   {'word': 'claw', 'tfidf': 0.499114829314058},\n",
              "   {'word': 'back', 'tfidf': 0.1892216338539946},\n",
              "   {'word': 'dwindling', 'tfidf': 0.4572386180709258},\n",
              "   {'word': 'band', 'tfidf': 0.3643421454792778},\n",
              "   {'word': 'reuters', 'tfidf': 0.24754017186645658},\n",
              "   {'word': 'bears', 'tfidf': 0.3372044607529448},\n",
              "   {'word': 'wall', 'tfidf': 0.5115985326511431},\n",
              "   {'word': 'ultra', 'tfidf': 0.4125512394225831},\n",
              "   {'word': 'st', 'tfidf': 0.2584728642725166},\n",
              "   {'word': 'sellers', 'tfidf': 0.4468379768438066},\n",
              "   {'word': 'black', 'tfidf': 0.2953171727366614},\n",
              "   {'word': 'short', 'tfidf': 0.2773120373951269},\n",
              "   {'word': 'seeing', 'tfidf': 0.37743394553516213},\n",
              "   {'word': 'street', 'tfidf': 0.24678348986493034}]),\n",
              " (1,\n",
              "  [{'word': 'investment', 'tfidf': 0.1890771769001148},\n",
              "   {'word': 'commercial', 'tfidf': 0.2057832028092643},\n",
              "   {'word': 'reputation', 'tfidf': 0.2578098186776328},\n",
              "   {'word': 'group', 'tfidf': 0.12468100563149095},\n",
              "   {'word': 'plays', 'tfidf': 0.22418048797172685},\n",
              "   {'word': 'reuters', 'tfidf': 0.1650267812443044},\n",
              "   {'word': 'firm', 'tfidf': 0.15969712503706046},\n",
              "   {'word': 'private', 'tfidf': 0.1929050573011279},\n",
              "   {'word': 'occasionally', 'tfidf': 0.33274321954270536},\n",
              "   {'word': 'market', 'tfidf': 0.13394932212703356},\n",
              "   {'word': 'part', 'tfidf': 0.16022031730914288},\n",
              "   {'word': 'bets', 'tfidf': 0.27861293130724324},\n",
              "   {'word': 'placed', 'tfidf': 0.2284965552404658},\n",
              "   {'word': 'looks', 'tfidf': 0.1973537176743789},\n",
              "   {'word': 'quietly', 'tfidf': 0.25188254045524316},\n",
              "   {'word': 'making', 'tfidf': 0.1698717076460444},\n",
              "   {'word': 'industry', 'tfidf': 0.15043731768548949},\n",
              "   {'word': 'aerospace', 'tfidf': 0.2581171817448437},\n",
              "   {'word': 'toward', 'tfidf': 0.1898997183872362},\n",
              "   {'word': 'carlyle', 'tfidf': 0.7168306746824437},\n",
              "   {'word': 'timed', 'tfidf': 0.324478643568105},\n",
              "   {'word': 'defense', 'tfidf': 0.1751279339938823},\n",
              "   {'word': 'well', 'tfidf': 0.17053284421704767},\n",
              "   {'word': 'another', 'tfidf': 0.14507889141437585},\n",
              "   {'word': 'controversial', 'tfidf': 0.20949395177306526}]),\n",
              " (2,\n",
              "  [{'word': 'doldrums', 'tfidf': 0.3770252270329423},\n",
              "   {'word': 'economy', 'tfidf': 0.3721400726458204},\n",
              "   {'word': 'summer', 'tfidf': 0.22694739048609625},\n",
              "   {'word': 'plus', 'tfidf': 0.24449073714833106},\n",
              "   {'word': 'cloud', 'tfidf': 0.295159450642955},\n",
              "   {'word': 'week', 'tfidf': 0.13121900794126834},\n",
              "   {'word': 'market', 'tfidf': 0.15069298739291276},\n",
              "   {'word': 'earnings', 'tfidf': 0.1792714404894228},\n",
              "   {'word': 'oil', 'tfidf': 0.13908157105107033},\n",
              "   {'word': 'expected', 'tfidf': 0.16094627131903613},\n",
              "   {'word': 'soaring', 'tfidf': 0.2596334462817101},\n",
              "   {'word': 'stocks', 'tfidf': 0.14976769101715193},\n",
              "   {'word': 'stock', 'tfidf': 0.17879168082328206},\n",
              "   {'word': 'prices', 'tfidf': 0.14472559202114177},\n",
              "   {'word': 'reuters', 'tfidf': 0.18565512889984243},\n",
              "   {'word': 'next', 'tfidf': 0.14062721303262238},\n",
              "   {'word': 'depth', 'tfidf': 0.31343954772064864},\n",
              "   {'word': 'worries', 'tfidf': 0.23009353850726894},\n",
              "   {'word': 'hang', 'tfidf': 0.30475018305843793},\n",
              "   {'word': 'outlook', 'tfidf': 0.4265073217271922},\n",
              "   {'word': 'crude', 'tfidf': 0.197241148492091}]),\n",
              " (3,\n",
              "  [{'word': 'exports', 'tfidf': 0.2146590164054526},\n",
              "   {'word': 'infrastructure', 'tfidf': 0.22959926718225876},\n",
              "   {'word': 'reuters', 'tfidf': 0.15913296762843637},\n",
              "   {'word': 'militia', 'tfidf': 0.2252006141545402},\n",
              "   {'word': 'official', 'tfidf': 0.15149485319300557},\n",
              "   {'word': 'halts', 'tfidf': 0.27365396741681164},\n",
              "   {'word': 'said', 'tfidf': 0.06593367258642661},\n",
              "   {'word': 'southern', 'tfidf': 0.336553609483104},\n",
              "   {'word': 'rebel', 'tfidf': 0.18209445014364567},\n",
              "   {'word': 'oil', 'tfidf': 0.35763832555989516},\n",
              "   {'word': 'showed', 'tfidf': 0.1743365558077232},\n",
              "   {'word': 'pipeline', 'tfidf': 0.4720829409342409},\n",
              "   {'word': 'iraq', 'tfidf': 0.23809526243476142},\n",
              "   {'word': 'authorities', 'tfidf': 0.18159366801541998},\n",
              "   {'word': 'export', 'tfidf': 0.23862435123782139},\n",
              "   {'word': 'main', 'tfidf': 0.36492623402353547},\n",
              "   {'word': 'flows', 'tfidf': 0.2774168429760197},\n",
              "   {'word': 'saturday', 'tfidf': 0.12197305137253434},\n",
              "   {'word': 'intelligence', 'tfidf': 0.20782569445751425},\n",
              "   {'word': 'halted', 'tfidf': 0.2557691357056513},\n",
              "   {'word': 'strike', 'tfidf': 0.17411586950893898}]),\n",
              " (4,\n",
              "  [{'word': 'posing', 'tfidf': 0.2589223867776184},\n",
              "   {'word': 'menace', 'tfidf': 0.5747440955975784},\n",
              "   {'word': 'afp', 'tfidf': 0.2559170042376607},\n",
              "   {'word': 'economy', 'tfidf': 0.14885602905832815},\n",
              "   {'word': 'presidential', 'tfidf': 0.1480257381794347},\n",
              "   {'word': 'record', 'tfidf': 0.1232987151692413},\n",
              "   {'word': 'tearaway', 'tfidf': 0.3918885216630942},\n",
              "   {'word': 'three', 'tfidf': 0.10314988960754677},\n",
              "   {'word': 'economic', 'tfidf': 0.14782686453681568},\n",
              "   {'word': 'world', 'tfidf': 0.09332201126546583},\n",
              "   {'word': 'oil', 'tfidf': 0.22253051368171256},\n",
              "   {'word': 'us', 'tfidf': 0.1669859687392097},\n",
              "   {'word': 'present', 'tfidf': 0.22209684830286883},\n",
              "   {'word': 'prices', 'tfidf': 0.23156094723382684},\n",
              "   {'word': 'records', 'tfidf': 0.19759033440942064},\n",
              "   {'word': 'new', 'tfidf': 0.1271397626254836},\n",
              "   {'word': 'straining', 'tfidf': 0.2904044404056468},\n",
              "   {'word': 'soar', 'tfidf': 0.2306791247647116},\n",
              "   {'word': 'elections', 'tfidf': 0.16009904796740967},\n",
              "   {'word': 'months', 'tfidf': 0.14002501854271598},\n",
              "   {'word': 'time', 'tfidf': 0.10623532598945136},\n",
              "   {'word': 'barely', 'tfidf': 0.21935019724396657},\n",
              "   {'word': 'toppling', 'tfidf': 0.27964532733021175},\n",
              "   {'word': 'wallets', 'tfidf': 0.2665151844733088}])]"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2\n"
      ],
      "metadata": {
        "id": "duN3S6SBmQDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv -O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-fGmlhImSWo",
        "outputId": "ffbe5865-b693-4932-f299-58367fac008b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1391  100  1391    0     0   3556      0 --:--:-- --:--:-- --:--:--  3557\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    22  100    22    0     0     61      0 --:--:-- --:--:-- --:--:--    61\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 61.9M  100 61.9M    0     0  19.2M      0  0:00:03  0:00:03 --:--:-- 19.2M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Begin spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SVM Data Load\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "Fd96vMqrm2Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the data directly into pyspark\n",
        "data_svm = spark.read.csv(\"data_for_svm.csv\", header=False, inferSchema=True)\n",
        "\n",
        "# Read the weights data using pandas; flatten to a 1d array\n",
        "w = pd.read_csv(\"w.csv\", header=None).values.flatten()\n",
        "\n",
        "# Read the bias data using pandas; take the item to extract the singular value contained within bias\n",
        "bias = pd.read_csv(\"bias.csv\", header=None).values.item()\n",
        "\n",
        "# Broadcast the weight and bias into read-only objects\n",
        "w= sc.broadcast(w)\n",
        "bias = sc.broadcast(bias)"
      ],
      "metadata": {
        "id": "WRBOVydWm3rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Separate the X and y data from data_svm to be conveniently used in calculations\n",
        "\n",
        "# Count the number of columns in data\n",
        "cols = len(data_svm.columns)\n",
        "\n",
        "# X values (all columns except the last in data_svm); create rdd\n",
        "X_rdd = data_svm.rdd.map(lambda input: np.array([input[i] for i in range(cols - 1)]))\n",
        "\n",
        "# Y values (last column from data_svm); create rdd\n",
        "y_rdd = data_svm.rdd.map(lambda input: input[cols - 1])\n"
      ],
      "metadata": {
        "id": "BDY7A0JUrOG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function\n",
        "\n",
        "def loss_SVM(w, b, X, y, lmb):\n",
        "    # Pair elements of x and y to easily track which y value corresponds to which x values\n",
        "    xy_rdd = X.zip(y)\n",
        "\n",
        "    # Map the calculations for the hinge value onto each row using the formula given in the assignment\n",
        "    hinge_value = xy_rdd.map(lambda pair: max(\n",
        "        0,\n",
        "        1 - pair[1] * (np.dot(w.value, pair[0]) + b.value)\n",
        "    ))\n",
        "\n",
        "    # Sum all hinge values, according to the given formula, using reduce\n",
        "    sum_hinge = hinge_value.reduce(lambda a, b: a + b)\n",
        "\n",
        "    # Count the number of rows in the data\n",
        "    n = xy_rdd.count()\n",
        "\n",
        "    # Calculate the average hinge value\n",
        "    avg_hinge = sum_hinge / n\n",
        "\n",
        "    # Calculate the L2 Norm\n",
        "    l2_norm = np.sum(w.value ** 2)\n",
        "\n",
        "    # Return the final value below according to the formula given in the assignment\n",
        "    return print(\"The value of the loss function is:\",round(avg_hinge + lmb * l2_norm, 5))\n",
        "\n"
      ],
      "metadata": {
        "id": "R1Ic2K5I1m_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the final value of the loss function using the given data\n",
        "loss_SVM(X = X_rdd,y= y_rdd,w = w, b = bias, lmb =1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-_uIE7Cz0nB",
        "outputId": "38a5c56a-102a-477b-a319-caa40b70a0da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of the loss function is: 1.00295\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the predictions using the svm classifier by mapping the formula given onto each row\n",
        "predict_classifier = X_rdd.map(lambda x: 1 if np.dot(w.value, x) + bias.value >= 0 else -1)\n",
        "\n",
        "# Print the first 15 predictions using the given data\n",
        "print(\"The first 15 predictions using the svm classifier are:\",predict_classifier.take(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayW9ucTOiT0P",
        "outputId": "818e7e17-96c8-4cad-f8a1-f4fcbab2e745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first 15 predictions using the svm classifier are: [-1, -1, -1, 1, -1, 1, -1, -1, 1, -1, 1, -1, -1, -1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uTUDGjOkVSYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Statement"
      ],
      "metadata": {
        "id": "O6BjEzNZTQVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, I used ChatGPT to help me determine how to create and group by keys and output the data in a way that both the TF-IDF value and document ID could be seen in a easy-to-read format in the first question when reducing. Specficially, this line of code was generated with the help of ChatGPT: tfidf_grouped = tfidf.groupByKey().mapValues(list).sortByKey(ascending=True).\n",
        "\n",
        "Additionally, I used ChatGPT in the second question to help with connecting corresponding x and y values in the formula. Chat recommended using the .zip function for this purpose (xy_rdd = X.zip(y)). Chat also recommended broadcasting my variables into a read-only format once I read my data for w and bias (w= sc.broadcast(w); bias = sc.broadcast(bias)). This helped to resolve errors that I was encountering without first broadcasting the variables. In the function itself, I learned that I must use \" w.value\" and \"b.value\" when referencing the weight and bias data from ChatGPT because of the broadcasting of this data."
      ],
      "metadata": {
        "id": "kuzuRCYbTTOe"
      }
    }
  ]
}